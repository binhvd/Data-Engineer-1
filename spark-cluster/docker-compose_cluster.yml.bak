version: "3.6"
services:
  # Master
  master-node:
    image: "binhvd/spark-cluster:0.2"
    command: bash -c "/home/big_data/spark-cmd.sh start master-node"
    hostname: master-node
    networks:
      - cluster-net
    ports:
      - target: 8088
        published: 8088
        protocol: tcp
        mode: host
      - target: 8080
        published: 8080
        protocol: tcp
        mode: host
      - target: 9870
        published: 9870
        protocol: tcp
        mode: host
      - target: 18080
        published: 18080
        protocol: tcp
        mode: host
    #volumes:
      # - "./data:/home/big_data/data" # Your data
      #- hdfs-master-data:/home/hadoop/data/nameNode
      #- hdfs-master-checkpoint-data:/home/hadoop/data/namesecondary
    deploy:
      mode: global # Required by Docker Swarm to make published ports work with other services
      endpoint_mode: dnsrr # Required to prevent java.net.ConnectException
      placement:
        constraints:
          - node.role==manager

  # Workers
  worker:
    image: "binhvd/spark-cluster:0.2"
    command: bash -c "/home/big_data/spark-cmd.sh start"
    networks:
      - cluster-net
    depends_on:
      - "master-node"
    #volumes:
    #  - hdfs-worker-data:/home/hadoop/data/dataNode
    deploy:
      mode: global
      placement:
        constraints:
          - node.role!=manager
   
  # Jupyter Notebook
  jupyter:
    image: "jupyter/pyspark-notebook:python-3.10.11"
    command: bash -c "start-notebook.sh --NotebookApp.token=''"
    networks:
      - cluster-net
    ports:
      - 10000:8888
    user: root
    environment:
      GRANT_SUDO: "yes" 
    deploy:
      mode: replicated
      replicas: 1      
      placement:
        constraints:
          - node.role==manager

#volumes:
  #hdfs-master-data:
  #hdfs-master-checkpoint-data:
  #hdfs-worker-data:

# Uses cluster-net network
networks:
  cluster-net:

